<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2020-06-13T20:18:00+05:30</updated><id>/feed.xml</id><title type="html">COEP DSAI</title><subtitle>Data Science and Artificial Intelligence Club at College of Engineering, Pune</subtitle><author><name>CoEP DSAI</name></author><entry><title type="html">Introduction to ARIMA Model</title><link href="/2020/05/15/arima-model.html" rel="alternate" type="text/html" title="Introduction to ARIMA Model" /><published>2020-05-15T00:00:00+05:30</published><updated>2020-05-15T00:00:00+05:30</updated><id>/2020/05/15/arima-model</id><content type="html" xml:base="/2020/05/15/arima-model.html">&lt;p&gt;Author: &lt;a href=&quot;https://www.linkedin.com/in/jinit-sanghvi-4329a016b/&quot;&gt;Jinit Sanghvi&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;introduction-to-arimamodel&quot;&gt;Introduction to ARIMA Model&lt;/h1&gt;

&lt;h2 id=&quot;a-brief-introduction-followed-by-implementation&quot;&gt;A brief introduction followed by implementation&lt;/h2&gt;
&lt;center&gt;
&lt;img src=&quot;https://www.freestockcharts.com/help/Content/Resources/Images/timeseriesforecast.png&quot; /&gt;
&lt;/center&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;Taken from &lt;a href=&quot;https://www.freestockcharts.com/&quot;&gt;freestockcharts.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ARIMA or Autoregressive Integrated Moving Average is a purely-regressive model and this model is used for forecasting values of a time series. A time series is essentially a sequence of data points or observations taken at different instances. Time Series are very common to find given how time-dependent most of the worldly schemes and variables are. To better understand this, take a look at the Stock Market or weather reports gathered over a timeline and you’ll observe patterns which are highly time-dependent.
&lt;br /&gt;
In this post, we’ll first cover the theory and then move on to the code&lt;/p&gt;

&lt;h3 id=&quot;theory&quot;&gt;Theory&lt;/h3&gt;

&lt;p&gt;One of the best ways to be introduced to the model is to understand why we’re using it, especially when other effective regression models like linear regression or multivariate regression exists. For time-series forecasting, why do we prefer the ARIMA model? Most of the time-series data available online focuses on the dependent variable, and how the dependent variable changes with time. For models like linear regression, we need independent variables to map a function from dependent variables to dependent variables for prediction.
However, this is not always possible because:-
&lt;br /&gt;&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;Independent Variables are not always available.&lt;/li&gt;
        &lt;li&gt;In quite a few scenarios, too many independent variables exist and we may find it difficult to find enough dependent variables to sufficiently explain the behavior of the time-series.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;i&gt;On the other hand, ARIMA leverages the correlation between the values of the time-series and time, allowing it to better understand the relation between independent variables and time. Since the model focuses more on patterns and behavior of the time-series instead of finding out which factors affect these dependent variables, the model is able to forecast on the basis of these recognized patterns and behavior.&lt;/i&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;i&gt;To better understand this model, let’s break it down into 2 parts and then integrate those 2 parts in the end.&lt;/i&gt;&lt;/p&gt;

&lt;h2 id=&quot;ar-or-autoregressive-model&quot;&gt;AR or Autoregressive Model&lt;/h2&gt;

&lt;p&gt;The intuition behind this model is that observations at different time-steps are related to the previous value of the variable. For example, on a hot sunny day, you predict that the next day will be hotter because you’ve noticed increasing temperatures. Similarly, the AR model finds the correlation between future time-steps and past time-steps to forecast future values.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y_t  = \alpha + \beta_1Y_{t-1} + \beta_2Y_{t-2} + \dots + \beta_pY_{t-p} + \epsilon_1&lt;/script&gt;

&lt;!--![AR Equation](https://latex.codecogs.com/gif.latex?Y_t%20%3D%20%5Calpha%20&amp;plus;%20%5Cbeta_1*Y_t_-_1%20&amp;plus;%20%5Cbeta_2*Y_t_-_2%20..%20&amp;plus;%20%5Cbeta_p*Y_t_-_p%20&amp;plus;%20%5Cepsilon_1)--&gt;
&lt;p&gt;&lt;em&gt;By the above equation, we can see how we can reduce this to a regression problem and statistically find the correlation between future values and earlier time-steps.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;moving-average-model&quot;&gt;Moving Average Model&lt;/h2&gt;

&lt;p&gt;The intuition behind this model in nature is of reinforcement, i.e, a moving average model tries to learn from the previous errors it has committed and tries to tweak itself accordingly. To better understand this, take a look at the equation below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y_t = \alpha + \epsilon_t + \phi_1\epsilon_{t - 1} + \phi_2\epsilon_{t-2} + \dots + \phi_q\epsilon_{t-q}&lt;/script&gt;

&lt;p&gt;&lt;small&gt;But, what does &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; signify? Simply put, it is the error or the difference between the actual value and the predicted value.&lt;/small&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Since this model tries to learn from its mistakes, it is better able to account for unpredictable changes in value and is able to correct itself to provide more accurate results and predictions.&lt;/i&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;Now that you’ve understood the Autoregressive Model and the Moving Averages model, it’s time to learn about ARIMA. When the Autoregressive Terms and the Moving Average terms are combined together with differencing to make the time-series stationary(more on this later), we get the ARIMA Model! Since the equation is regressive in nature, we can find the respective weights of the terms in the equation using regression techniques.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Y_t = \alpha + \beta_1Y_{t-1} + \beta_2Y_{t-2} + \dots + \beta_pY_{t-p} + \phi_1\epsilon_{t - 1} + \phi_2\epsilon_{t-2} + \dots + \phi_q\epsilon_{t-q}&lt;/script&gt;
&lt;!--![ARIMA Equation](https://latex.codecogs.com/gif.latex?Y_t%20%3D%20%5Calpha%20&amp;plus;%20%5Cbeta%20_1Y_t_-_1%20&amp;plus;%20%5Cbeta%20_2Y_t_-_2%20&amp;plus;%20....%20&amp;plus;%20%5Cbeta%20_pY_t_-_p%20&amp;plus;%20%5Cphi%20_1%5Cepsilon_t_-_1%20&amp;plus;%20%5Cphi%20_2%5Cepsilon_t_-_2%20&amp;plus;%20....%20&amp;plus;%20%5Cphi%20_q%5Cepsilon_t_-_q)--&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;So far, we’ve understood the basic intuition behind the ARIMA Model. Let’s dig a bit deeper and understand the parameters of an ARIMA model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Consider a list below, and assume that every successive element of the list is a successive time-step or observation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[1, 3, 5, 4]&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Now, when we difference the list, we subtract the nth value of the series with the (n-1)th value of the series. For a better understanding:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;After First Differencing:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[3–1, 5–3, 4–5] = [2, 2, -1]&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;After Second Differencing:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[2–2, -1–2] = [0, -3]&lt;/script&gt;

&lt;p&gt;The reason why we difference the time-series is to make the time-series stationary, i.e, the mean and the variance of the time-series remains constant/stable over time which allows us to reduce components like trend and seasonality(illustrated later). This is important because ARIMA expects the time-series to be stationary. Thus, we keep differencing the time-series till it becomes stationary.&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Now that we have understood all the underlying concepts that we’re going to utilize, let’s learn how to find these parameters and dive right into the code!&lt;/i&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To begin with, let’s start with some basic imports.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;statsmodels.api&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'axes.labelsize'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xtick.labelsize'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ytick.labelsize'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text.color'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'k'&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;statsmodels.tsa.arima_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ARIMA&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For this post, I’m going to use a time-series from the &lt;em&gt;Huge Stock Market Dataset&lt;/em&gt;. This is just a sample time-series used to further ground the concepts you read about. At the end of this post, you’ll be able to apply these concepts to any other time-series you want to.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;aus1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a.us.txt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;','&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index_col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parse_dates&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;aus1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
  &lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;

  &lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Open&lt;/th&gt;
      &lt;th&gt;High&lt;/th&gt;
      &lt;th&gt;Low&lt;/th&gt;
      &lt;th&gt;Close&lt;/th&gt;
      &lt;th&gt;Volume&lt;/th&gt;
      &lt;th&gt;OpenInt&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Date&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1999-11-18&lt;/th&gt;
      &lt;td&gt;30.713&lt;/td&gt;
      &lt;td&gt;33.754&lt;/td&gt;
      &lt;td&gt;27.002&lt;/td&gt;
      &lt;td&gt;29.702&lt;/td&gt;
      &lt;td&gt;66277506&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1999-11-19&lt;/th&gt;
      &lt;td&gt;28.986&lt;/td&gt;
      &lt;td&gt;29.027&lt;/td&gt;
      &lt;td&gt;26.872&lt;/td&gt;
      &lt;td&gt;27.257&lt;/td&gt;
      &lt;td&gt;16142920&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1999-11-22&lt;/th&gt;
      &lt;td&gt;27.886&lt;/td&gt;
      &lt;td&gt;29.702&lt;/td&gt;
      &lt;td&gt;27.044&lt;/td&gt;
      &lt;td&gt;29.702&lt;/td&gt;
      &lt;td&gt;6970266&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1999-11-23&lt;/th&gt;
      &lt;td&gt;28.688&lt;/td&gt;
      &lt;td&gt;29.446&lt;/td&gt;
      &lt;td&gt;27.002&lt;/td&gt;
      &lt;td&gt;27.002&lt;/td&gt;
      &lt;td&gt;6332082&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1999-11-24&lt;/th&gt;
      &lt;td&gt;27.083&lt;/td&gt;
      &lt;td&gt;28.309&lt;/td&gt;
      &lt;td&gt;27.002&lt;/td&gt;
      &lt;td&gt;27.717&lt;/td&gt;
      &lt;td&gt;5132147&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;aus1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
  &lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;

  &lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Open&lt;/th&gt;
      &lt;th&gt;High&lt;/th&gt;
      &lt;th&gt;Low&lt;/th&gt;
      &lt;th&gt;Close&lt;/th&gt;
      &lt;th&gt;Volume&lt;/th&gt;
      &lt;th&gt;OpenInt&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;4521.000000&lt;/td&gt;
      &lt;td&gt;4521.000000&lt;/td&gt;
      &lt;td&gt;4521.000000&lt;/td&gt;
      &lt;td&gt;4521.000000&lt;/td&gt;
      &lt;td&gt;4.521000e+03&lt;/td&gt;
      &lt;td&gt;4521.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;27.856296&lt;/td&gt;
      &lt;td&gt;28.270442&lt;/td&gt;
      &lt;td&gt;27.452486&lt;/td&gt;
      &lt;td&gt;27.871357&lt;/td&gt;
      &lt;td&gt;3.993503e+06&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;12.940880&lt;/td&gt;
      &lt;td&gt;13.176000&lt;/td&gt;
      &lt;td&gt;12.711735&lt;/td&gt;
      &lt;td&gt;12.944389&lt;/td&gt;
      &lt;td&gt;2.665730e+06&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;7.223100&lt;/td&gt;
      &lt;td&gt;7.513900&lt;/td&gt;
      &lt;td&gt;7.087800&lt;/td&gt;
      &lt;td&gt;7.323800&lt;/td&gt;
      &lt;td&gt;0.000000e+00&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;19.117000&lt;/td&gt;
      &lt;td&gt;19.435000&lt;/td&gt;
      &lt;td&gt;18.780000&lt;/td&gt;
      &lt;td&gt;19.089000&lt;/td&gt;
      &lt;td&gt;2.407862e+06&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;24.456000&lt;/td&gt;
      &lt;td&gt;24.809000&lt;/td&gt;
      &lt;td&gt;24.159000&lt;/td&gt;
      &lt;td&gt;24.490000&lt;/td&gt;
      &lt;td&gt;3.460621e+06&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;36.502000&lt;/td&gt;
      &lt;td&gt;37.046000&lt;/td&gt;
      &lt;td&gt;35.877000&lt;/td&gt;
      &lt;td&gt;36.521000&lt;/td&gt;
      &lt;td&gt;4.849809e+06&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;105.300000&lt;/td&gt;
      &lt;td&gt;109.370000&lt;/td&gt;
      &lt;td&gt;97.881000&lt;/td&gt;
      &lt;td&gt;107.320000&lt;/td&gt;
      &lt;td&gt;6.627751e+07&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;aus1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;DatetimeIndex(['1999-11-18', '1999-11-19', '1999-11-22', '1999-11-23',
               '1999-11-24', '1999-11-26', '1999-11-29', '1999-11-30',
               '1999-12-01', '1999-12-02',
               ...
               '2017-10-30', '2017-10-31', '2017-11-01', '2017-11-02',
               '2017-11-03', '2017-11-06', '2017-11-07', '2017-11-08',
               '2017-11-09', '2017-11-10'],
              dtype='datetime64[ns]', name='Date', length=4521, freq=None)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Before we use this time-series, we first need to resample it into a time-series wherein each observation differs by a month.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aus1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Open'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'MS'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aus1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Close'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'MS'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;DatetimeIndex(['1999-11-01', '1999-12-01', '2000-01-01', '2000-02-01',
               '2000-03-01', '2000-04-01', '2000-05-01', '2000-06-01',
               '2000-07-01', '2000-08-01',
               ...
               '2017-02-01', '2017-03-01', '2017-04-01', '2017-05-01',
               '2017-06-01', '2017-07-01', '2017-08-01', '2017-09-01',
               '2017-10-01', '2017-11-01'],
              dtype='datetime64[ns]', name='Date', length=217, freq='MS')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;So far, so good! Let’s plot the time-series and see what it looks like!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebooks/2020-05-15-arima-code_files/2020-05-15-arima-code_10_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Now that we’ve seen what it looks like, let’s try breaking it down into its components. Let’s try extracting the trend, the seasonality and the residual of our time-series.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pylab&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'figure.figsize'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decomposition&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tsa&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seasonal_decompose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'additive'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decomposition&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebooks/2020-05-15-arima-code_files/2020-05-15-arima-code_12_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Our inference from breaking the time-series down into components like seasonality and trend is that the given time-series is not stationary. Since the trend and seasonality of the time-series affect its values at different time-steps, this time-series is not stationary as its mean and variance keeps changing. Thus, we have to difference the time-series at least once to make it stationary.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you remember, we discussed the three parameters of the ARIMA Model. In order to determine these 3 parameters and finding the best combination of these 3 parameters in order to yield the best results, we’ll use a combination of statistical tools and iteration (trying out different models to check which achieves the best results). &lt;/p&gt;

&lt;p&gt;Let’s start by determining &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;. As we mentioned earlier, we’ll keep differencing the time-series until it becomes stationary. Let’s try differencing it once and use the Augmented Dickey Fuller Test to find if it is stationary or not.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;statsmodels.tsa.stattools&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;adfuller&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y1_d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y1_d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1_d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropna&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;adfuller&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1_d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ADF Statistic: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'p-value: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ADF Statistic: -2.955284
p-value: 0.039287
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;As we see that the &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;-value obtained is less than 0.05, we can reject the null hypothesis and say that the series is stationary. We can also see that the series has become more stationary from the plot above. Thus, obtaining a &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;-value of less than 0.05 using ADF is a good indicator that our time-series has become stationary.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y1_d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebooks/2020-05-15-arima-code_files/2020-05-15-arima-code_18_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In order to determine &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, we need to first understand what an ACF plot is. Autocorrelation Function plot or ACF plot shows how correlated the variable is with its previous time-steps. For example, while predicting stock prices, the ACF plot tries to show how correlated the stock prices of March are to the stock prices of February, how likely are the stock prices in March to follow the behavior of stock prices in February. To better understand this, let’s plot it.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;statsmodels.graphics.tsaplots&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot_acf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_pacf&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plot_acf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1_d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lags&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebooks/2020-05-15-arima-code_files/2020-05-15-arima-code_21_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebooks/2020-05-15-arima-code_files/2020-05-15-arima-code_21_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;We can see that the auto-correlation is strong for the first 2 lags and then it decays and oscillates in the blue region.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Inferring from this, we can see that the first 1–2 lags show high correlation and values keep decreasing in the blue region.&lt;/em&gt;&lt;br /&gt;
Thus, we can say that the value of &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; will lie in the range spanning from 1 to 2.&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;Let’s try determining &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; now. The process of determining &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; is very similar to the process of determining &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;. Instead of using ACF, we’ll now use a PACF, or a Partial Autocorrelation Function. A PACF plot shows how correlated a variable is with itself at a previous time-step, ignoring all linear dependencies it has on time-steps that lie between the time-step against which correlation is to be found and the current time-step. Also called a conditional correlation fucntion, PACF aims to find the correlation between two time-steps independent of all other time-steps in between. Let’s understand this better with a plot.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plot_pacf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1_d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lags&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebooks/2020-05-15-arima-code_files/2020-05-15-arima-code_25_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebooks/2020-05-15-arima-code_files/2020-05-15-arima-code_25_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;From the plot we can see that the plot cuts off to zero at the second lag so we can estimate that the value of &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; will be lesser than 2.&lt;/em&gt;&lt;br /&gt;
Thus, we can say that the value &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; shall lie in the range spanning from 0 to 1.&lt;/p&gt;
&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;You’ve finally learnt the necessary building blocks required to create an ARIMA model in order to perform time-series forecasting! Let’s jump right into it!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We divide our time-frame into a training and a testing set so that we can later gauge how well our model is performing. Training data is 70% of the original time-series data and testing data is 30% of the original time-series data.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;series&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Alright, now let’s start training the model! Let’s try using values of &lt;script type=&quot;math/tex&quot;&gt;(p,d,q)&lt;/script&gt; equal to (2,1,1) and see how the model performs!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;arima_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ARIMA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;series&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arima_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;disp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forecast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;actual_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;series&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;actual_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Test error is {}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Test error is 3.273019781729989





[&amp;lt;matplotlib.lines.Line2D at 0x1d39598f668&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebooks/2020-05-15-arima-code_files/2020-05-15-arima-code_31_2.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We can see that the model has learnt the nature and behavior of the time-series, and is performing pretty well on the test set!&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Let’s try out new values of &lt;script type=&quot;math/tex&quot;&gt;(p,d,q)&lt;/script&gt;. Let’s try out (1,1,1) this time and see if we obtain better results.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;series&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;arima_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ARIMA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;series&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arima_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;disp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forecast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;actual_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;series&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;actual_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Test error is {}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Test error is 2.355571525013936





[&amp;lt;matplotlib.lines.Line2D at 0x1d3959f32b0&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/notebooks/2020-05-15-arima-code_files/2020-05-15-arima-code_35_2.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Great, we have obtained better results by changing the parameters of the model! By using the statistical methods above and by trying out different values, you can achieve great results on various different time-series.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;I hope you’re now acquainted with various concepts of Time-Series Forecasting and the ARIMA Model!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You can get the jupyter notebook corresponding to this blog &lt;a href=&quot;/assets/notebooks/2020-05-15-arima-code.ipynb&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>CoEP DSAI</name></author><summary type="html">Author: Jinit Sanghvi</summary></entry><entry><title type="html">Adversarial Reprogramming</title><link href="/2020/04/21/adversarial-reprogramming.html" rel="alternate" type="text/html" title="Adversarial Reprogramming" /><published>2020-04-21T00:00:00+05:30</published><updated>2020-04-21T00:00:00+05:30</updated><id>/2020/04/21/adversarial-reprogramming</id><content type="html" xml:base="/2020/04/21/adversarial-reprogramming.html">&lt;p&gt;Author: &lt;a href=&quot;https://www.linkedin.com/in/gouri-n-7a364017a/&quot;&gt;Gouri Nangliya&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Neural Networks find extensive applications in areas where traditional computers don’t fare too well. Today, neural networks are used for solving many business problems such as sales forecasting, customer research, data validation, and risk management. So, can you fool neural network? Sounds fun, right! This blog explains one of the techniques to fool neural network into doing something it was not intended to do. Basically, this blog is a summary of paper titled &lt;strong&gt;ADVERSARIAL REPROGRAMMING OF NEURAL NETWORKS.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;INTRODUCTION&lt;/h3&gt;

&lt;p&gt;Well, it seems deep neural networks are susceptible to adversarial attacks. An adversarial attack consists of subtly modifying an original image in such a way that the changes are almost undetectable to the human eye. The modified image is called an adversarial image, and when submitted to a classifier is misclassified, while the original one is correctly classified. Adversarial examples are such inputs to model that attacker has intentionally created so that model can make mistake. This is done by taking an image from dataset and using gradient descent to search for a nearby image on which model makes mistake. To make things more clear, look at example below : a model classifies image as “panda” with 57.7% confidence(image on left). But, when little &lt;em&gt;carefully crafted noise&lt;/em&gt; is added, same model classifies new image as “gibbon” with 99.3% confidence(image on right). Now, you can clearly see, we can’t differentiate between image on left and image on right. But, this tricks the model!&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1000/1*PmCgcjO3sr3CPPaCpy5Fgw.png&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;One such concept is &lt;strong&gt;Adversarial Reprogramming of neural networks&lt;/strong&gt; - in which model performs a task choosen by attacker which it was not intended to perform. Attacker just add slight change in all test inputs and reprogram the model to perform the new task. Attacks can be either a model is targeted to classify as a single output for all(or majority) of inputs or just degrade the performance of the network by misclassifying the inputs.&lt;/p&gt;

&lt;p&gt;Consider a model trained to perform some original task: for inputs $x$, it produces output $f(x)$. Consider an adversary(attacker) who wishes to perform an adversarial task: for  inputs $x’$ (not in domain of $x$), it wishes to produce an output $g(x’)$. So, this task can be done by using adversarial reprogramming functions, $h_f(.;\theta)$ and $h_g(.;\theta)$ where $h_f$ converts input from $x’$ into domain of $x$ and $h_g$ converts outputs i.e $f(h (.;\theta))$ to $g(x’)$. 
Now, we compute $\theta$ to get the equation:
$h_g(f (h_f (x’))$ = $g(x’)$. 
Here, $\theta$ is known as &lt;em&gt;adversarial reprogram.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this blog, we will look at adversarial reprogramming for &lt;strong&gt;MNIST classification&lt;/strong&gt;. So, the idea is, we embedded MNIST digits of size 28 × 28 × 3 inside the original CIFAR-10 image, assign the first 10 CIFAR-10 labels to the MNIST digits, and train an adversarial program. So, in this case, $x$ will be CIFAR-10 images and $x’$ will be MNIST images, $f(x)$ will be classifying CIFAR-10 images into one of the 10 labels and $g(x)$ as you can guess will be classifying MNIST images into 0-9 i.e. again 10 labels. Results show that it can be successfully reprogrammed to function as an MNIST classifier.&lt;/p&gt;

&lt;h3 id=&quot;diving-into-maths-behind-this&quot;&gt;Diving into Maths behind this!&lt;/h3&gt;

&lt;p&gt;Adversarial reprogram is something added to input. Consider ImageNet classifer for CIFAR10. Adversarial reprogram is defined as :
$P = \tanh( W \odot M )$ where
$W \in \mathbb{R}^{nxnx3}$ is the parameter to be learned 
n is CIFAR10 image width 
M is masking matrix in which 0 - adversarial data for new task, 1 - otherwise.
$\tanh(\odot)$ is needed to make perbutations in (-1,1) i.e the same range of images in which model is trained to classify.&lt;/p&gt;

&lt;p&gt;The adversarial image(new input) becomes :
$X_{adv} = h_f(x’;W) = X’ + P$ where
$x’ \in \mathbb{R}^{nxnx3}$ is sample of MNIST dataset
$X’ \in \mathbb{R}^{nxnx3}$ is equivalent CIFAR10 image which is obtained by placing $x’$ at the centre.&lt;/p&gt;

&lt;p&gt;Let, $P(y|X)$ be the probability that it predicted $y$ for the given input image $X$ where $y \in {1,…,1000}$. Now, we define $h_g(y_{adv})$ be the hard-coded mapping function that maps $y_{adv}$ to set of ImageNet labels. So, our goal is to maximize $P(h_g(y_{adv})|X_{adv})$. Theresore, optimization problem can be seen as :
$W’ = \underset{W}{argmin}(-logP(h_g(y_{adv})|X_{adv}) + \lambda||W||^2_F)$ 
where $\lambda$ is regularization coefficient to reduce overfitting. So, attacker only need to add this data and store the program, majority of computation will be done by targeted network.&lt;/p&gt;

&lt;h3 id=&quot;how-to-build-mnist-classifier&quot;&gt;How to build MNIST classifier?&lt;/h3&gt;

&lt;p&gt;Follow steps below:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Take a pretrained model on ImageNet like ResNet. If you are using CIFAR-10 dataset, then don’t use pretrained, weights don’t transfer well.&lt;/li&gt;
  &lt;li&gt;Add last fully connected layer for 10 output classes(0-9).&lt;/li&gt;
  &lt;li&gt;Train the model on CIFAR-10.&lt;/li&gt;
  &lt;li&gt;Add an &lt;em&gt;adversarial program&lt;/em&gt; image to your MNIST image and pass that through the model. Map the outputs of model using the remapping you chose above to get your MNIST predictions.&lt;/li&gt;
  &lt;li&gt;Train only the adversarial program image on the remapped labels, while keeping the ResNet weights frozen.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now you got yourself an MNIST classifier: Take an MNIST image, add on your trained adversarial program, run it through ResNet, and remap its labels to get predictions for MNIST.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://venturebeat.com/wp-content/uploads/2018/07/Capture-boring.png?w=1200&amp;amp;strip=all4&quot; /&gt;
&lt;/center&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;p&gt;The key point in bulding this MNIST is that we need to optimize in image space so that classification loss decreases. Actual size of CIFAR10 is 32x32 and that of MNIST image is 28x28. After implementing this MNIST classifier, I tried different sizes for MNIST images which is to be placed at the center of CIFAR10 images. Take a look at adversarial examples below for 14x14 MNIST images. As you can see, the MNIST image of size 14x14 placed at center of 32x32 image and rest of image space has some well crafted noise which helps us to build MNIST classifier.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://i.ibb.co/y4SFQD4/input.png&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;Below are the accuracy plots for learning rates 0.01 and 0.1 respectively.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://i.ibb.co/4mKyBSK/adv-blog0-01.png&quot; /&gt;
&lt;/center&gt;

&lt;center&gt;
&lt;img src=&quot;https://i.ibb.co/fHNCVcy/adv-blog0-1.png&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;So, we can see from the figure that as size of MNIST image is decreasing, accuracy is increasing. This is because the area in which model needs to optimize i.e. (32x32 - adversarial image area) is increasing and so is the accuracy. The drop of accuarcy at size 7x7 is due the fact that for odd number, my implementation doesn’t place adversarial image at center. So, this reveals another property that by placing the adversarial image at center, we get more accuracy than placing at any other location.&lt;/p&gt;

&lt;p&gt;So, this is how you can fool a neural network. Adversarial examples make machine learning models vulnerable to attacks, as in the following scenarios. A self-driving car crashes into another car because it ignores a stop sign. Someone had placed a picture over the sign, which looks like a stop sign with a little dirt for humans, but was designed to look like a parking prohibition sign for the sign recognition software of the car. A spam detector fails to classify an email as spam. The spam mail has been designed to resemble a normal email, but with the intention of cheating the recipient. A machine-learning powered scanner scans suitcases for weapons at the airport. A knife was developed to avoid detection by making the system think it is an umbrella.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt; :&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Adversarial Reprogramming of Neural Networks&lt;/em&gt; - &lt;a href=&quot;https://arxiv.org/pdf/1806.11146.pdf&quot;&gt;https://arxiv.org/pdf/1806.11146.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@ml.at.berkeley/tricking-neural-networks-create-your-own-adversarial-examples-a61eb7620fd8&quot;&gt;https://medium.com/@ml.at.berkeley/tricking-neural-networks-create-your-own-adversarial-examples-a61eb7620fd8&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/adversarial-examples-in-deep-learning-be0b08a94953&quot;&gt;https://towardsdatascience.com/adversarial-examples-in-deep-learning-be0b08a94953&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rajatvd.github.io/Exploring-Adversarial-Reprogramming/&quot;&gt;https://rajatvd.github.io/Exploring-Adversarial-Reprogramming/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/onfido-tech/adversarial-attacks-and-defences-for-convolutional-neural-networks-66915ece52e7&quot;&gt;https://medium.com/onfido-tech/adversarial-attacks-and-defences-for-convolutional-neural-networks-66915ece52e7&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>CoEP DSAI</name></author><summary type="html">Author: Gouri Nangliya</summary></entry><entry><title type="html">Object Detection and Instance Segmentation: A Detailed Overview</title><link href="/2020/04/06/object-detection-and-instance-segmentation-overview.html" rel="alternate" type="text/html" title="Object Detection and Instance Segmentation: A Detailed Overview" /><published>2020-04-06T00:00:00+05:30</published><updated>2020-04-06T00:00:00+05:30</updated><id>/2020/04/06/object-detection-and-instance-segmentation-overview</id><content type="html" xml:base="/2020/04/06/object-detection-and-instance-segmentation-overview.html">&lt;p&gt;Author: &lt;a href=&quot;https://in.linkedin.com/in/shaunak-halbe-565a0716b&quot;&gt;Shaunak Halbe&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Object Detection is by far one of the most important fields of research in Computer Vision. Researchers have for a long time been interested in this field, but significant results were produced in the recent years owing to the rise of Convnets as feature extractors and Transfer Learning as method of passing on previous knowledge. Early object detectors were based on handcrafted features, and employed a sliding window based approach which was computationally inefficient and less accurate. Modern techniques include Region Proposal Methods, Single Shot Methods, Anchor Free Methods and so on.&lt;/p&gt;

&lt;h4 id=&quot;a-object-detection&quot;&gt;A) Object Detection&lt;/h4&gt;
&lt;p&gt;Object Detection refers to the method of identifying and correctly labeling all the objects present in the image frame.&lt;/p&gt;

&lt;p&gt;This broadly consists of two steps :&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Object Localization&lt;/strong&gt; : Here, a bounding box or enclosing region is determined in the tightest possible manner in order to locate the exact position of the object in the image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Image Classification&lt;/strong&gt;: The localized object is then fed to a classifier which labels the object.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*pWoHu_uUDebBSSNmyMydLQ.png&quot; /&gt;
&lt;/center&gt;

&lt;h4 id=&quot;b-semantic-segmentation&quot;&gt;B) Semantic Segmentation&lt;/h4&gt;

&lt;p&gt;It refers to the process of linking each pixel in the given image to a particular class label. For example in the following image the pixels are labelled as car, tree, pedestrian etc. These segments are then used to find the interactions / relations between various objects.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*5jK-um9WuXJhN8d0HjSNbg.jpeg&quot; /&gt;
&lt;/center&gt;

&lt;h4 id=&quot;c-instance-segmentation&quot;&gt;C) Instance Segmentation&lt;/h4&gt;
&lt;p&gt;Here, we associate a class label to each pixel similar to semantic segmentation, except that it treats multiple objects of the same class as individual objects / separate entities.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*XGpOlTBTS3SBKdvsoQoZ8g.png&quot; /&gt;
&lt;/center&gt;

&lt;h4 id=&quot;d-panoptic-segmentation&quot;&gt;D) Panoptic Segmentation&lt;/h4&gt;
&lt;p&gt;It is a combination of Instance and Semantic Segmentation in a way that we associate with each pixel two values: Its class label and a instance number. It also recognizes the sky, the road, and other background elements collectively known as stuff.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*u_d3V4ppQr2Cht0BvYpbbg.jpeg&quot; /&gt;
&lt;/center&gt;

&lt;h2 id=&quot;important-concepts&quot;&gt;Important Concepts&lt;/h2&gt;

&lt;h3 id=&quot;bounding-boxes&quot;&gt;Bounding Boxes&lt;/h3&gt;
&lt;p&gt;It is a tight rectangle used to enclose the object of interest. It is generally described by four values – &lt;script type=&quot;math/tex&quot;&gt;(bx, by, bh, bw)&lt;/script&gt; Where &lt;script type=&quot;math/tex&quot;&gt;(bx, by)&lt;/script&gt; are the co-ordinates of the center of the box and &lt;script type=&quot;math/tex&quot;&gt;(bh, bw)&lt;/script&gt; are the height and width of the box respectively measured on a scale of &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*m4h0ng0rfMVYPSY93WxDpw.png&quot; /&gt;
&lt;/center&gt;

&lt;h3 id=&quot;anchor-boxes&quot;&gt;Anchor Boxes&lt;/h3&gt;
&lt;p&gt;These are a set of predefined bounding boxes of a certain height and width. These boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect and are typically chosen based on object sizes in your training data-sets. During detection, the predefined anchor boxes are tiled across the image. The network predicts the probability and other attributes, such as background, intersection over union (IoU) and offsets for every tiled anchor box. The predictions are used to refine each individual anchor box. You can define several anchor boxes, each for a different object size.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*qnVj5T81pGHSMBDdzv5nAA.png&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;Thus the network refines these anchor boxes to finally output the tight bounding boxes. These are defined by the scale and aspect ratio.
Here,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Aspect Ratio is the width / height of the box.&lt;/li&gt;
  &lt;li&gt;Size is the height and width of the box. eg (&lt;script type=&quot;math/tex&quot;&gt;256 \times 256&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;Scale is the multiplying factor of the required box w.r.t to base box&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intersection-over-union-iou&quot;&gt;Intersection over Union (IOU)&lt;/h3&gt;

&lt;p&gt;It is an evaluation metric used to check the accuracy of the predicted bounding box w.r.t the actual ground truth.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2LPQLE87SJBRCSXhpow9sA.png&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;An IOU &lt;script type=&quot;math/tex&quot;&gt;&gt; 0.5&lt;/script&gt; is considered as a good prediction and is used for further evaluation.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*JoyOqNm42i90sM2Y7VWBYA.png&quot; /&gt;
&lt;/center&gt;

&lt;h3 id=&quot;non-max-suppression&quot;&gt;Non-max suppression&lt;/h3&gt;
&lt;p&gt;If multiple boxes are present for a given object then, as the name suggests, this technique discards all boxes except the one having the maximum IOU.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*LHMO63plv3iPp_0oQQcu5A.png&quot; /&gt;
&lt;/center&gt;

&lt;h3 id=&quot;binary-mask&quot;&gt;Binary Mask&lt;/h3&gt;
&lt;p&gt;It is a 2D array, that has a data point representing the same pixel width &amp;amp; height of the image.
Each pixel in our mask is labeled either a &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt; (&lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;false&lt;/code&gt;) for whether or not it belongs to the predicted instance.&lt;/p&gt;

&lt;center&gt;
&lt;figure&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*4iU-yNAuESbgMKOytGs46g.jpeg&quot; /&gt;
&lt;figcaption class=&quot;imageCaption&quot;&gt;Binary Mask for a cat&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;

&lt;h3 id=&quot;metric--mean-average-precision&quot;&gt;Metric - Mean Average Precision&lt;/h3&gt;

&lt;p&gt;Mean Average Precision or &lt;script type=&quot;math/tex&quot;&gt;mAP&lt;/script&gt; is the metric used to quantify the accuracy of object detectors.&lt;/p&gt;

&lt;p&gt;Firstly,&lt;/p&gt;

&lt;!--center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Ifj2xS6bT16DWpHBwE21BQ.png&quot;&gt;
&lt;/center--&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\text{precision} = \frac{\text{true positives}}{\text{true positives} + \text{false positives}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Average precision for a image means precision averaged over all instances of objects present in the image.
&lt;script type=&quot;math/tex&quot;&gt;mAP&lt;/script&gt; is the average precision averaged over IOU of &lt;script type=&quot;math/tex&quot;&gt;0.5&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;0.95&lt;/script&gt; with a step size of &lt;script type=&quot;math/tex&quot;&gt;0.05&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;As a convention, &lt;script type=&quot;math/tex&quot;&gt;mAP&lt;/script&gt; is expressed as a percent value.&lt;/p&gt;

&lt;h3 id=&quot;region-proposals&quot;&gt;Region Proposals&lt;/h3&gt;

&lt;h4 id=&quot;a-rcnn&quot;&gt;A) RCNN&lt;/h4&gt;
&lt;p&gt;The RCNN is a region proposal based object detection algorithm. Its stands for “&lt;strong&gt;R&lt;/strong&gt;egion based &lt;strong&gt;C&lt;/strong&gt;onvolutional &lt;strong&gt;N&lt;/strong&gt;eural &lt;strong&gt;N&lt;/strong&gt;etwork”.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*vX4wUj_XMk17nYaC82lhBQ.jpeg&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;Steps involved :&lt;/p&gt;

&lt;p&gt;The original RCNN paper [1] by Girshick et. al. Uses the Selective Search method for generating around 2000 region proposals.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;b&gt;Selective Search&lt;/b&gt;: Selective Search uses a Hierarchical Grouping Algorithm to generate the region proposals.

&lt;ol&gt;
&lt;li&gt;
&lt;u&gt;Generating Initial Regions&lt;/u&gt;:

It first runs a graph based image segmentation algorithm to obtain the initial regions as seen in the leftmost column of the image below.

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*CZj5NjOZKndKIS8iqn_nJw.jpeg&quot; /&gt;
&lt;/center&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;u&gt;Similarity Measure&lt;/u&gt;:
We find the similarity between regions based color, texture, size and shape compatibility:
A Similarity Metric is obtained as follows:
&lt;span&gt;
$$s(r_i,r_j) = a_1 * S_{colour}(r_i,r_j) + a_2 * S_{texture}(r_i,r_j) + \\ a_3 * S_{size}(r_i,r_j) + a_4 * S_{fill}(r_i,r_j)$$
&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;u&gt;Recursive Grouping&lt;/u&gt;:
Starting from these initial regions we recursively group these regions based on the similarity metric. We stop once the required number or proposals is attained.
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;b&gt;Warping&lt;/b&gt;: Each of the region proposal is resized(scaled) to the required input size of the Convnet and enclosed in a tight box.
&lt;/li&gt;
&lt;li&gt;
&lt;b&gt;Feature Extraction&lt;/b&gt;: Each of these warped region is fed one y one to a Convnet which outputs a 4096 length feature vector. 
&lt;/li&gt;
&lt;li&gt; 
&lt;b&gt;Classification&lt;/b&gt;: The 4096 length feature vector is then fed to a SVM which classifies whether a object is present and assigns a label to it.
&lt;/li&gt;
&lt;li&gt;
&lt;b&gt;Bounding Box Regressor&lt;/b&gt;: In addition to the class label the rcnn uses a linear regressor which outputs the bounding box co-ordinates for the object. 
&lt;/li&gt;
&lt;li&gt;
&lt;b&gt;IOU and non-max suppression&lt;/b&gt;: In case of overlaps the best scored region is chosen and the rest are discarded. 
&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;b-fast-rcnn&quot;&gt;B) Fast RCNN&lt;/h4&gt;

&lt;p&gt;It is an improvised version of the RCNN as it eliminates some of the shortcomings of the rcnn.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Higher detection quality (&lt;script type=&quot;math/tex&quot;&gt;mAP&lt;/script&gt;) than R-CNN, SPPnet&lt;/li&gt;
  &lt;li&gt;Time of Computation is reduced as it is a single stage process.&lt;/li&gt;
  &lt;li&gt;Does not require any extra disk storage to cache intermediate features.&lt;/li&gt;
  &lt;li&gt;Lesser parameters as compared to rcnn and SPPnet.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Process&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;b&gt;Feature Map Generation&lt;/b&gt;: Entire image is fed in along with object proposals to a Convnet. On passing through the Conv layers and Max Pooling layers a feature map is obtained.
&lt;/li&gt;
&lt;li&gt;&lt;b&gt;ROI Pooling&lt;/b&gt;: The Region of Interest (ROI) in the feature map is given by \(r,c,h,w\) co-ordinates. This ROI is then passed through a ROI pooling layer to get a \(H \times W\) feature map.
&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Fully Connected Layers&lt;/b&gt;: This feature map is then extracted to a FC layer and then passed through FC layers to a softmax for class probability prediction and a regressor for the bounding box regression outputs.
&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Zirwh-FDdgXX2fz7Qgy_mA.jpeg&quot; /&gt;
&lt;/center&gt;

&lt;h4 id=&quot;c-faster-rcnn&quot;&gt;C) Faster RCNN&lt;/h4&gt;

&lt;p&gt;Faster RCNN model was proposed by Ross Girshick et. al. [3] as a computationally efficient solution to object detection.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Merits over Fast RCNN&lt;/b&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It eliminates the computational bottleneck of determining region proposals from a image.&lt;/li&gt;
  &lt;li&gt;It uses a Fully Convolutional neural network for this purpose which makes it a single flow pipeline.&lt;/li&gt;
  &lt;li&gt;The RPN introduced in this paper[3] shares the features with the object detector as well.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;Architecture and Operation&lt;/b&gt;:&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*NXWE7BHug0i-FQlHo5xa7w.png&quot; /&gt;
&lt;/center&gt;

&lt;ol&gt;
  &lt;li&gt;Feature Map Generation: The image is passed through Conv layers which output a feature map.&lt;/li&gt;
  &lt;li&gt;Region Proposal Network: A sliding window is used in RPN for each location over the feature map.&lt;/li&gt;
  &lt;li&gt;Anchors: For each location, &lt;script type=&quot;math/tex&quot;&gt;k (k=9)&lt;/script&gt; anchor boxes are used (3 scales of 128, 256 and 512, and 3 aspect ratios of 1:1, 1:2, 2:1) for generating region proposals.&lt;/li&gt;
  &lt;li&gt;Classification : A &lt;code class=&quot;highlighter-rouge&quot;&gt;clslayer&lt;/code&gt; outputs &lt;script type=&quot;math/tex&quot;&gt;2k&lt;/script&gt; scores whether there is object or not for &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; boxes.&lt;/li&gt;
  &lt;li&gt;Regression: A &lt;code class=&quot;highlighter-rouge&quot;&gt;reglayer&lt;/code&gt; outputs &lt;script type=&quot;math/tex&quot;&gt;4k&lt;/script&gt; for the coordinates(box center coordinates, width and height) of &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; boxes.&lt;/li&gt;
  &lt;li&gt;Detection network: Except for the RPN part the Detection network is the same as that of Fast rcnn.&lt;/li&gt;
  &lt;li&gt;Alternate Training: The RPN and Detection part are trained alternately so that they share the features learnt by each other.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;d-mask-rcnn&quot;&gt;D) Mask RCNN&lt;/h4&gt;

&lt;p&gt;Mask RCNN extends Faster Rcnn by adding a parallel mask output branch. It is a very important method used in instance segmentation.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Motivation&lt;/b&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Faster Rcnn, Yolo and other object detection algorithms output a bounding box and a class probability label associated with that box.&lt;/li&gt;
  &lt;li&gt;We as humans do not locate real life objects by drawing boxes around them, instead we look at the outline and the pose of the object in order to detect it.&lt;/li&gt;
  &lt;li&gt;In this regard, mask rcnn gets closer to human style of object perception.&lt;/li&gt;
  &lt;li&gt;The research on mask rcnn motivates us further leading to areas of panoptic segmentation, person keypoint detection, sports pose estimation etc.&lt;/li&gt;
  &lt;li&gt;All the self driving cars use the fundamental concept behind mask rcnn.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;Architecture and Implementation&lt;/b&gt;:&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Vlnu9esS7GSi9l8Yvo1XRw.jpeg&quot; /&gt;
&lt;/center&gt;
&lt;ol&gt;
  &lt;li&gt;Mask R-CNN adopts the same two-stage procedure, with an identical first stage (which is RPN).&lt;/li&gt;
  &lt;li&gt;In the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask for each RoI.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;ROI Align Layer&lt;/b&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The ROI pool layer in Faster Rcnn performs quantizations like flooring the floating point values and aggregation functions like Maxpool.&lt;/li&gt;
  &lt;li&gt;Such operations result into coarse features destroying the finer pixel to pixel arrangements which are necessary for instance segmentation.&lt;/li&gt;
  &lt;li&gt;To Counter this, Mask Rcnn uses the ROI Align layer which uses bi-linear interpolation instead of quantization which preserves the pixel alignments and improves mask accuracy.&lt;/li&gt;
&lt;/ol&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*HuzsE1q4zzHYzeb9gOrPJA.jpeg&quot; /&gt;
&lt;/center&gt;

&lt;h2 id=&quot;current-research-and-future-scope&quot;&gt;Current Research and Future Scope&lt;/h2&gt;

&lt;p&gt;Panoptic Segmentation: Recent CVPR papers make use of the mask rcnn model and build on top of it to achieve state of the art results on popular data-sets like City-Scapes.&lt;/p&gt;

&lt;p&gt;Mesh Rcnn: It is a very accurate system proposed by Georgia Gkioxari et. al. used for 3-D shape prediction which augments the mask rcnn models with a mesh prediction branch to generate voxel representations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt; :&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014&lt;/li&gt;
  &lt;li&gt;R. Girshick. Fast R-CNN. In ICCV, 2015&lt;/li&gt;
  &lt;li&gt;Faster R-CNN: To-wards real-time object detection with region proposal net-works. In NIPS, 2015&lt;/li&gt;
  &lt;li&gt;Kaiming He, Georgia Gkioxari, Piotr Doll ́ar, and Ross Girshick. Mask R-CNN. In ICCV, 2017&lt;/li&gt;
  &lt;li&gt;Image References: Google&lt;/li&gt;
&lt;/ol&gt;</content><author><name>CoEP DSAI</name></author><summary type="html">Author: Shaunak Halbe</summary></entry></feed>