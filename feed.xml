<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-08T18:40:00+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">COEP DSAI</title><subtitle>Data Science and Artificial Intelligence Club at College of Engineering, Pune</subtitle><author><name>CoEP DSAI</name></author><entry><title type="html">Object Detection and Instance Segmentation: A Detailed Overview</title><link href="http://localhost:4000/2020/04/06/object-detection-and-instance-segmentation-overview.html" rel="alternate" type="text/html" title="Object Detection and Instance Segmentation: A Detailed Overview" /><published>2020-04-06T00:00:00+05:30</published><updated>2020-04-06T00:00:00+05:30</updated><id>http://localhost:4000/2020/04/06/object-detection-and-instance-segmentation-overview</id><content type="html" xml:base="http://localhost:4000/2020/04/06/object-detection-and-instance-segmentation-overview.html">&lt;p&gt;Author: &lt;a href=&quot;https://in.linkedin.com/in/shaunak-halbe-565a0716b&quot;&gt;Shaunak Halbe&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Object Detection is by far one of the most important fields of research in Computer Vision. Researchers have for a long time been interested in this field, but significant results were produced in the recent years owing to the rise of Convnets as feature extractors and Transfer Learning as method of passing on previous knowledge. Early object detectors were based on handcrafted features, and employed a sliding window based approach which was computationally inefficient and less accurate. Modern techniques include Region Proposal Methods, Single Shot Methods, Anchor Free Methods and so on.&lt;/p&gt;

&lt;h4 id=&quot;a-object-detection&quot;&gt;A) Object Detection&lt;/h4&gt;
&lt;p&gt;Object Detection refers to the method of identifying and correctly labeling all the objects present in the image frame.&lt;/p&gt;

&lt;p&gt;This broadly consists of two steps :&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Object Localization&lt;/strong&gt; : Here, a bounding box or enclosing region is determined in the tightest possible manner in order to locate the exact position of the object in the image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Image Classification&lt;/strong&gt;: The localized object is then fed to a classifier which labels the object.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*pWoHu_uUDebBSSNmyMydLQ.png&quot; /&gt;
&lt;/center&gt;

&lt;h4 id=&quot;b-semantic-segmentation&quot;&gt;B) Semantic Segmentation&lt;/h4&gt;

&lt;p&gt;It refers to the process of linking each pixel in the given image to a particular class label. For example in the following image the pixels are labelled as car, tree, pedestrian etc. These segments are then used to find the interactions / relations between various objects.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*5jK-um9WuXJhN8d0HjSNbg.jpeg&quot; /&gt;
&lt;/center&gt;

&lt;h4 id=&quot;c-instance-segmentation&quot;&gt;C) Instance Segmentation&lt;/h4&gt;
&lt;p&gt;Here, we associate a class label to each pixel similar to semantic segmentation, except that it treats multiple objects of the same class as individual objects / separate entities.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*XGpOlTBTS3SBKdvsoQoZ8g.png&quot; /&gt;
&lt;/center&gt;

&lt;h4 id=&quot;d-panoptic-segmentation&quot;&gt;D) Panoptic Segmentation&lt;/h4&gt;
&lt;p&gt;It is a combination of Instance and Semantic Segmentation in a way that we associate with each pixel two values: Its class label and a instance number. It also recognizes the sky, the road, and other background elements collectively known as stuff.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*u_d3V4ppQr2Cht0BvYpbbg.jpeg&quot; /&gt;
&lt;/center&gt;

&lt;h2 id=&quot;important-concepts&quot;&gt;Important Concepts&lt;/h2&gt;

&lt;h3 id=&quot;bounding-boxes&quot;&gt;Bounding Boxes&lt;/h3&gt;
&lt;p&gt;It is a tight rectangle used to enclose the object of interest. It is generally described by four values – &lt;script type=&quot;math/tex&quot;&gt;(bx, by, bh, bw)&lt;/script&gt; Where &lt;script type=&quot;math/tex&quot;&gt;(bx, by)&lt;/script&gt; are the co-ordinates of the center of the box and &lt;script type=&quot;math/tex&quot;&gt;(bh, bw)&lt;/script&gt; are the height and width of the box respectively measured on a scale of &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*m4h0ng0rfMVYPSY93WxDpw.png&quot; /&gt;
&lt;/center&gt;

&lt;h3 id=&quot;anchor-boxes&quot;&gt;Anchor Boxes&lt;/h3&gt;
&lt;p&gt;These are a set of predefined bounding boxes of a certain height and width. These boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect and are typically chosen based on object sizes in your training data-sets. During detection, the predefined anchor boxes are tiled across the image. The network predicts the probability and other attributes, such as background, intersection over union (IoU) and offsets for every tiled anchor box. The predictions are used to refine each individual anchor box. You can define several anchor boxes, each for a different object size.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*qnVj5T81pGHSMBDdzv5nAA.png&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;Thus the network refines these anchor boxes to finally output the tight bounding boxes. These are defined by the scale and aspect ratio.
Here,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Aspect Ratio is the width / height of the box.&lt;/li&gt;
  &lt;li&gt;Size is the height and width of the box. eg (&lt;script type=&quot;math/tex&quot;&gt;256 \times 256&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;Scale is the multiplying factor of the required box w.r.t to base box&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intersection-over-union-iou&quot;&gt;Intersection over Union (IOU)&lt;/h3&gt;

&lt;p&gt;It is an evaluation metric used to check the accuracy of the predicted bounding box w.r.t the actual ground truth.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2LPQLE87SJBRCSXhpow9sA.png&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;An IOU &lt;script type=&quot;math/tex&quot;&gt;&gt; 0.5&lt;/script&gt; is considered as a good prediction and is used for further evaluation.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*JoyOqNm42i90sM2Y7VWBYA.png&quot; /&gt;
&lt;/center&gt;

&lt;h3 id=&quot;non-max-suppression&quot;&gt;Non-max suppression&lt;/h3&gt;
&lt;p&gt;If multiple boxes are present for a given object then, as the name suggests, this technique discards all boxes except the one having the maximum IOU.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*LHMO63plv3iPp_0oQQcu5A.png&quot; /&gt;
&lt;/center&gt;

&lt;h3 id=&quot;binary-mask&quot;&gt;Binary Mask&lt;/h3&gt;
&lt;p&gt;It is a 2D array, that has a data point representing the same pixel width &amp;amp; height of the image.
Each pixel in our mask is labeled either a &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt; (&lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;false&lt;/code&gt;) for whether or not it belongs to the predicted instance.&lt;/p&gt;

&lt;center&gt;
&lt;figure&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*4iU-yNAuESbgMKOytGs46g.jpeg&quot; /&gt;
&lt;figcaption class=&quot;imageCaption&quot;&gt;Binary Mask for a cat&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;

&lt;h3 id=&quot;metric--mean-average-precision&quot;&gt;Metric - Mean Average Precision&lt;/h3&gt;

&lt;p&gt;Mean Average Precision or &lt;script type=&quot;math/tex&quot;&gt;mAP&lt;/script&gt; is the metric used to quantify the accuracy of object detectors.&lt;/p&gt;

&lt;p&gt;Firstly,&lt;/p&gt;

&lt;!--center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Ifj2xS6bT16DWpHBwE21BQ.png&quot;&gt;
&lt;/center--&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\text{precision} = \frac{\text{true positives}}{\text{true positives} + \text{false positives}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Average precision for a image means precision averaged over all instances of objects present in the image.
&lt;script type=&quot;math/tex&quot;&gt;mAP&lt;/script&gt; is the average precision averaged over IOU of &lt;script type=&quot;math/tex&quot;&gt;0.5&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;0.95&lt;/script&gt; with a step size of &lt;script type=&quot;math/tex&quot;&gt;0.05&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;As a convention, &lt;script type=&quot;math/tex&quot;&gt;mAP&lt;/script&gt; is expressed as a percent value.&lt;/p&gt;

&lt;h3 id=&quot;region-proposals&quot;&gt;Region Proposals&lt;/h3&gt;

&lt;h4 id=&quot;a-rcnn&quot;&gt;A) RCNN&lt;/h4&gt;
&lt;p&gt;The RCNN is a region proposal based object detection algorithm. Its stands for “&lt;strong&gt;R&lt;/strong&gt;egion based &lt;strong&gt;C&lt;/strong&gt;onvolutional &lt;strong&gt;N&lt;/strong&gt;eural &lt;strong&gt;N&lt;/strong&gt;etwork”.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*vX4wUj_XMk17nYaC82lhBQ.jpeg&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;Steps involved :&lt;/p&gt;

&lt;p&gt;The original RCNN paper [1] by Girshick et. al. Uses the Selective Search method for generating around 2000 region proposals.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;b&gt;Selective Search&lt;/b&gt;: Selective Search uses a Hierarchical Grouping Algorithm to generate the region proposals.

&lt;ol&gt;
&lt;li&gt;
&lt;u&gt;Generating Initial Regions&lt;/u&gt;:

It first runs a graph based image segmentation algorithm to obtain the initial regions as seen in the leftmost column of the image below.

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*CZj5NjOZKndKIS8iqn_nJw.jpeg&quot; /&gt;
&lt;/center&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;u&gt;Similarity Measure&lt;/u&gt;:
We find the similarity between regions based color, texture, size and shape compatibility:
A Similarity Metric is obtained as follows:
&lt;span&gt;
$$s(r_i,r_j) = a_1 * S_{colour}(r_i,r_j) + a_2 * S_{texture}(r_i,r_j) + \\ a_3 * S_{size}(r_i,r_j) + a_4 * S_{fill}(r_i,r_j)$$
&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;u&gt;Recursive Grouping&lt;/u&gt;:
Starting from these initial regions we recursively group these regions based on the similarity metric. We stop once the required number or proposals is attained.
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;b&gt;Warping&lt;/b&gt;: Each of the region proposal is resized(scaled) to the required input size of the Convnet and enclosed in a tight box.
&lt;/li&gt;
&lt;li&gt;
&lt;b&gt;Feature Extraction&lt;/b&gt;: Each of these warped region is fed one y one to a Convnet which outputs a 4096 length feature vector. 
&lt;/li&gt;
&lt;li&gt; 
&lt;b&gt;Classification&lt;/b&gt;: The 4096 length feature vector is then fed to a SVM which classifies whether a object is present and assigns a label to it.
&lt;/li&gt;
&lt;li&gt;
&lt;b&gt;Bounding Box Regressor&lt;/b&gt;: In addition to the class label the rcnn uses a linear regressor which outputs the bounding box co-ordinates for the object. 
&lt;/li&gt;
&lt;li&gt;
&lt;b&gt;IOU and non-max suppression&lt;/b&gt;: In case of overlaps the best scored region is chosen and the rest are discarded. 
&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;b-fast-rcnn&quot;&gt;B) Fast RCNN&lt;/h4&gt;

&lt;p&gt;It is an improvised version of the RCNN as it eliminates some of the shortcomings of the rcnn.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Higher detection quality (&lt;script type=&quot;math/tex&quot;&gt;mAP&lt;/script&gt;) than R-CNN, SPPnet&lt;/li&gt;
  &lt;li&gt;Time of Computation is reduced as it is a single stage process.&lt;/li&gt;
  &lt;li&gt;Does not require any extra disk storage to cache intermediate features.&lt;/li&gt;
  &lt;li&gt;Lesser parameters as compared to rcnn and SPPnet.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Process&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;b&gt;Feature Map Generation&lt;/b&gt;: Entire image is fed in along with object proposals to a Convnet. On passing through the Conv layers and Max Pooling layers a feature map is obtained.
&lt;/li&gt;
&lt;li&gt;&lt;b&gt;ROI Pooling&lt;/b&gt;: The Region of Interest (ROI) in the feature map is given by \(r,c,h,w\) co-ordinates. This ROI is then passed through a ROI pooling layer to get a \(H \times W\) feature map.
&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Fully Connected Layers&lt;/b&gt;: This feature map is then extracted to a FC layer and then passed through FC layers to a softmax for class probability prediction and a regressor for the bounding box regression outputs.
&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Zirwh-FDdgXX2fz7Qgy_mA.jpeg&quot; /&gt;
&lt;/center&gt;

&lt;h4 id=&quot;c-faster-rcnn&quot;&gt;C) Faster RCNN&lt;/h4&gt;

&lt;p&gt;Faster RCNN model was proposed by Ross Girshick et. al. [3] as a computationally efficient solution to object detection.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Merits over Fast RCNN&lt;/b&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It eliminates the computational bottleneck of determining region proposals from a image.&lt;/li&gt;
  &lt;li&gt;It uses a Fully Convolutional neural network for this purpose which makes it a single flow pipeline.&lt;/li&gt;
  &lt;li&gt;The RPN introduced in this paper[3] shares the features with the object detector as well.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;Architecture and Operation&lt;/b&gt;:&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*NXWE7BHug0i-FQlHo5xa7w.png&quot; /&gt;
&lt;/center&gt;

&lt;ol&gt;
  &lt;li&gt;Feature Map Generation: The image is passed through Conv layers which output a feature map.&lt;/li&gt;
  &lt;li&gt;Region Proposal Network: A sliding window is used in RPN for each location over the feature map.&lt;/li&gt;
  &lt;li&gt;Anchors: For each location, &lt;script type=&quot;math/tex&quot;&gt;k (k=9)&lt;/script&gt; anchor boxes are used (3 scales of 128, 256 and 512, and 3 aspect ratios of 1:1, 1:2, 2:1) for generating region proposals.&lt;/li&gt;
  &lt;li&gt;Classification : A &lt;code class=&quot;highlighter-rouge&quot;&gt;clslayer&lt;/code&gt; outputs &lt;script type=&quot;math/tex&quot;&gt;2k&lt;/script&gt; scores whether there is object or not for &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; boxes.&lt;/li&gt;
  &lt;li&gt;Regression: A &lt;code class=&quot;highlighter-rouge&quot;&gt;reglayer&lt;/code&gt; outputs &lt;script type=&quot;math/tex&quot;&gt;4k&lt;/script&gt; for the coordinates(box center coordinates, width and height) of &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; boxes.&lt;/li&gt;
  &lt;li&gt;Detection network: Except for the RPN part the Detection network is the same as that of Fast rcnn.&lt;/li&gt;
  &lt;li&gt;Alternate Training: The RPN and Detection part are trained alternately so that they share the features learnt by each other.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;d-mask-rcnn&quot;&gt;D) Mask RCNN&lt;/h4&gt;

&lt;p&gt;Mask RCNN extends Faster Rcnn by adding a parallel mask output branch. It is a very important method used in instance segmentation.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Motivation&lt;/b&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Faster Rcnn, Yolo and other object detection algorithms output a bounding box and a class probability label associated with that box.&lt;/li&gt;
  &lt;li&gt;We as humans do not locate real life objects by drawing boxes around them, instead we look at the outline and the pose of the object in order to detect it.&lt;/li&gt;
  &lt;li&gt;In this regard, mask rcnn gets closer to human style of object perception.&lt;/li&gt;
  &lt;li&gt;The research on mask rcnn motivates us further leading to areas of panoptic segmentation, person keypoint detection, sports pose estimation etc.&lt;/li&gt;
  &lt;li&gt;All the self driving cars use the fundamental concept behind mask rcnn.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;Architecture and Implementation&lt;/b&gt;:&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Vlnu9esS7GSi9l8Yvo1XRw.jpeg&quot; /&gt;
&lt;/center&gt;
&lt;ol&gt;
  &lt;li&gt;Mask R-CNN adopts the same two-stage procedure, with an identical first stage (which is RPN).&lt;/li&gt;
  &lt;li&gt;In the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask for each RoI.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;ROI Align Layer&lt;/b&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The ROI pool layer in Faster Rcnn performs quantizations like flooring the floating point values and aggregation functions like Maxpool.&lt;/li&gt;
  &lt;li&gt;Such operations result into coarse features destroying the finer pixel to pixel arrangements which are necessary for instance segmentation.&lt;/li&gt;
  &lt;li&gt;To Counter this, Mask Rcnn uses the ROI Align layer which uses bi-linear interpolation instead of quantization which preserves the pixel alignments and improves mask accuracy.&lt;/li&gt;
&lt;/ol&gt;

&lt;center&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*HuzsE1q4zzHYzeb9gOrPJA.jpeg&quot; /&gt;
&lt;/center&gt;

&lt;h2 id=&quot;current-research-and-future-scope&quot;&gt;Current Research and Future Scope&lt;/h2&gt;

&lt;p&gt;Panoptic Segmentation: Recent CVPR papers make use of the mask rcnn model and build on top of it to achieve state of the art results on popular data-sets like City-Scapes.&lt;/p&gt;

&lt;p&gt;Mesh Rcnn: It is a very accurate system proposed by Georgia Gkioxari et. al. used for 3-D shape prediction which augments the mask rcnn models with a mesh prediction branch to generate voxel representations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt; :&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014&lt;/li&gt;
  &lt;li&gt;R. Girshick. Fast R-CNN. In ICCV, 2015&lt;/li&gt;
  &lt;li&gt;Faster R-CNN: To-wards real-time object detection with region proposal net-works. In NIPS, 2015&lt;/li&gt;
  &lt;li&gt;Kaiming He, Georgia Gkioxari, Piotr Doll ́ar, and Ross Girshick. Mask R-CNN. In ICCV, 2017&lt;/li&gt;
  &lt;li&gt;Image References: Google&lt;/li&gt;
&lt;/ol&gt;</content><author><name>CoEP DSAI</name></author><summary type="html">Author: Shaunak Halbe</summary></entry></feed>